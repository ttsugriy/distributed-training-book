---
title: "Phase Transitions in Scaling"
subtitle: "When Different Factors Dominate"
---

::: {.chapter-opener}
Scaling is not smooth. There are regimes where model capacity dominates, others where optimizer noise dominates, and transitions between them.
:::

::: {.investigation-question}
**The Question**: Why do scaling laws sometimes break? What causes the "phase transitions" observed in compute-optimal training?
:::

## The 4+3 Phases

Recent work identifies multiple phases in the data-complexity/target-complexity plane:

1. **Model-capacity limited**: Model too small for the task
2. **Optimizer-noise limited**: Gradient noise dominates learning
3. **Feature-embedding limited**: Representation bottleneck
4. **Compute-optimal**: Balanced allocation

*[Detailed analysis to be completed]*

## Exercises

*[To be completed]*
